{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Qual o objetivo do comando cache em Spark?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O comando cache() salva o resultado de uma operação em memória. Este comando é útil quando se deseja usar o resultado de alguma operação mais de uma vez, evitando reprocessamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos principais motivos é o fato de o Spark conseguir diminuir a quantidade de leitura e escrita em disco, utilizando a memória de maneira mais eficiente que um MapReduce.\n",
    "Quando executamos um comando em Spark, ele monta uma espécie de plano de execução sobre os dados (DAG). O plano é construído de modo a minimizar a transferência de dados e a leitura e escrita em disco. A ordem lógica de execuções é também levada em consideração, verificando-se as dependências entre cada etapa, o que também otimiza a execução. O plano de ação também determina a melhor maneira de paralelizar as tarefas dentro do cluster, o que é uma das grandes vantagens em se usar o Spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qual é a função do SparkContext?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O SparkContext é a interface de interação com o ambiente Spark. Através dele determinam-se diversas configurações de ambiente local, cluster, sistema de arquivos e etc. Ele acaba sendo uma espécie de orquestrador de uma aplicação Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explique com suas palavras o que é Resilient Distributed Datasets(RDD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O RDD é a estrutura de dados base do Spark. É a representação lógica usada para representar os datasets a serem manipulados em um ambiente Spark. O RDD determina como os dados são distribuídos através do cluster e suas partições."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ReduceByKey, o Spark faz uma espécie de join dos dados de saída com as chaves correspondentes em cada partição antes de transferir os dados entre partições. Já GroupByKey, todos os pares chave-valor são transferidos entre as partições, sem remover nenhuma redundância. Assim, com o GroupByKey há muito mais transferência de dados entre o cluster e mais I/O, e para grandes datasets, ambos acabam sendo um gargalo de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explique o que o código Scala abaixo faz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "val textFile = sc.textFile(\"hdfs://...\")\n",
    "val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                .map(word => (word, 1)) \n",
    "                .reduceByKey(_ + _)\n",
    "counts.saveAsTextFile(\"hdfs://...\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É basicamente um word count, usando map reduce.\n",
    "\n",
    "Primeiro, carrega um arquivo do hdfs para o RDD. Em seguinda, quebra o texto a cada caracter de espaço(' '), ou seja, está separando cada palavra individual do texto.\n",
    "O map armazena na variável counts a quantidade de vezes em que cada palavra única aparece no texto.\n",
    "O último passo é escrever em um arquivo hdfs o resultado dessa operação, ou seja, a frequência com que cada palavra aparece no texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio dataset \"HTTP requests to the NASA Kennedy Space Center WWW server\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando e limpando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de linhas: 3461613\n",
      "Número de linhas dentro do formato válido: 3461612\n",
      "Número de linhas inválidas: 1\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('nasa-http-requests-dataset').setMaster('local').set('spark.executor.cores','1')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "raw_log_files = sc.textFile(\"NASA_access_log_*\")\n",
    "\n",
    "def parseLog(line):\n",
    "    \n",
    "    # A regex abaixo considera linhas com o seguinte padrão (exemplo retirado do próprio dataset):\n",
    "    # 199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
    "    # Qualquer linha que não dê match com a regex será considerada inválida\n",
    "    \n",
    "    regex = '^(\\S+) (-) (-) \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\S+)'\n",
    "\n",
    "    match = re.search(regex, line)\n",
    "    if match is None:\n",
    "        return (line, 0)\n",
    "    return (Row(\n",
    "        hostname       = match.group(1),\n",
    "        #dash1         = match.group(2),\n",
    "        #dash2         = match.group(3),\n",
    "        date           = match.group(4).split(':')[0], #o comando split remove horário e timezone, já que estes não são necessários para o teste\n",
    "        request        = match.group(5),\n",
    "        return_code    = match.group(6),\n",
    "        bytesize       = match.group(7)\n",
    "    ), 1)\n",
    "\n",
    "\n",
    "parsed_lines = raw_log_files.map(lambda line: parseLog(line)).cache()\n",
    "\n",
    "print('Número total de linhas: {}'.format(parsed_lines.count()))\n",
    "\n",
    "clean_lines = (parsed_lines\n",
    "                   .filter(lambda s: s[1] == 1)\n",
    "                   .map(lambda s: s[0])\n",
    "                   .cache())\n",
    "\n",
    "print('Número de linhas dentro do formato válido: {}'.format(clean_lines.count()))\n",
    "\n",
    "\n",
    "failed_lines = (parsed_lines\n",
    "                   .filter(lambda s: s[1] == 0)\n",
    "                   .map(lambda s: s[0]))\n",
    "\n",
    "print('Número de linhas inválidas: {}'.format(failed_lines.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daqui em diante consideraremos apenas as linhas que estão no formato esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1: Qual é o número de hosts únicos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de hosts únicos: 137978\n"
     ]
    }
   ],
   "source": [
    "hosts = clean_lines.map(lambda log: (log.hostname, 1))\n",
    "uniqueHosts = hosts.distinct()\n",
    "\n",
    "print('Número de hosts únicos: {}'.format(uniqueHosts.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2: qual é a quantidade total de erros 404?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de erros 404: 20901\n"
     ]
    }
   ],
   "source": [
    "code_404 = (clean_lines.filter(lambda log: log.return_code == '404')).cache()\n",
    "\n",
    "print('Quantidade de erros 404: {}'.format(code_404.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3: Quais são os 5 requests que mais causaram erro 404?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os 5 Requests que mais causaram erro 404 : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('GET /pub/winvn/readme.txt HTTP/1.0', 2004),\n",
       " ('GET /pub/winvn/release.txt HTTP/1.0', 1732),\n",
       " ('GET /shuttle/missions/STS-69/mission-STS-69.html HTTP/1.0', 682),\n",
       " ('GET /shuttle/missions/sts-68/ksc-upclose.gif HTTP/1.0', 426),\n",
       " ('GET /history/apollo/a-001/a-001-patch-small.gif HTTP/1.0', 384)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests_resulting_in_404_code = code_404.map(lambda log: (log.request, 1))\n",
    "sum_by_request_404 = requests_resulting_in_404_code.reduceByKey(lambda a, b: a + b)\n",
    "top5_requests = sum_by_request_404.takeOrdered(5, lambda s: -1 * s[1])\n",
    "print('Os 5 Requests que mais causaram erro 404 : ')\n",
    "top5_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4: Qual é a quantidade de erros 404 por dia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'code_404' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-545a2fa581ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_404\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdate_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Quantidade de erros 404 por dia: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdate_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'code_404' is not defined"
     ]
    }
   ],
   "source": [
    "dates = code_404.map(lambda log: (log.date, 1))\n",
    "date_sum = dates.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print('Quantidade de erros 404 por dia: ')\n",
    "date_sum.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 5: Qual é o total de bytes retornados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de bytes retornados: 65524314915.0\n"
     ]
    }
   ],
   "source": [
    "filter_bitesize_zero = (clean_lines.filter(lambda log: log.bytesize != '-')).cache()\n",
    "df = spark.createDataFrame(filter_bitesize_zero)\n",
    "total_bytes = df.agg({'bytesize':'sum'}).collect()\n",
    "\n",
    "print('Total de bytes retornados: {}'.format(total_bytes[0][0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
